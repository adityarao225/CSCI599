<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<title>assignment3</title>
		<style>
			body { margin: 0; }
		</style>
    <script type="importmap">
        {
          "imports": {
            "three": "https://unpkg.com/three@0.147.0/build/three.module.js",
            "three/addons/": "https://unpkg.com/three@0.147.0/examples/jsm/"
          }
        }
    </script>
	</head>
	<body>
		<h1 style="text-align: center;">Assignment 3</h1>
		<h2>Introduction</h2>
		<p>For this assignment, you will be training your own NeRF or Gaussian Splatting. 
			<ul>
				<li><a href="https://github.com/bmild/nerf">NeRF</a></li>
				<li><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3DGS</a></li>
			</ul>
			We have made available a visualization tool using the Three.js library implemented in "./js/assignment3.js" and an example NerfStudio Colab can be found <a href="https://colab.research.google.com/drive/13ZUOVo9N4fjZcoS_R9N0BsYa2kV6ywuq?usp=sharing">here</a>. You have the option to launch the demo either through Colab or on your local machine. It is important to note that while Colab offers free TPU access, completing the training within the free tier may not be feasible. In this assignment, your task is to collect your own data, conduct calibration, and train your NeRF/3DGS model. You are welcome to use any programming language you are comfortable with. Ensure that the final results are exported in obj/ply format for visualization.  You should then evaluate your results by comparing them with solutions from <a href="https://alicevision.org/#meshroom">Mesh Room</a> or <a href="https://colmap.github.io/">COLMAP</a>.
			<br><br>
			<b>How to Submit: </b>Please submit this template file along with your implementation as a zip file. The zip file should contain your source code, the generated results in PLY mesh format, and a report that has been modified using this HTML file. The report should comprise your results and a concise explanation of your implementation. Alternatively, you may choose to create a GitHub repository containing all these elements and provide a link for submission.
			<br><br>
			<b>Requirements / Rubric: </b>The grading is based on the correctness of your implementation. You are encouraged to use the visualization tool to debug your implementation. You can also use the visualization tool to test your implementation on other 3D models. </p>
				<ul>
					<li>+80 pts: Complete the training using your own data. </li>
					<li>+20 pts: Write up your project, algorithms, reporting results (reprojection error) and visualisations, compare your reconstruction with open source software Colmap.</li>
					<li>+10 pts: Extra credit (see below)</li>
					<li>-5*n pts: Lose 5 points for every time (after the first) you do not follow the instructions for the hand in format</li>
				</ul>
			<b>Extract Credit:</b> You are free to complete any extra credit:
				<ul>
					<li>up to 5 pts: Present results with your own captured data.</li>
					<li>up to 10 pts: Train your models with both Nerf and 3DGS.</li>
					<li>up to 20 pts: Train your model with language embedding (e.g., LERF).</li>
				</ul>
		</p>
		<h2>NeRF/3DGS</h2>
		<h3>Training NeRF on buldozer pre built dataset</h3>
		<iframe width="560" height="315" src="../plots/output.mp4" frameborder="0" allowfullscreen></iframe>
		<h3>Training NeRF on my custom video</h3>
		<iframe width="560" height="315" src="../plots/output_nerf.mp4" frameborder="0" allowfullscreen></iframe>
		<h3>Extra Credit: Training 3DGS on my custom data</h3>
		<iframe width="560" height="315" src="../plots/output_gaussian.mp4" frameborder="0" allowfullscreen></iframe>

		<h3>MY OUTPUT - Point Cloud - NeRF (Custom Data) below</h3>
		<p> some of the point cloud points have been edited to focus on the main object of concern.</p>
        <div id="container1"></div>
		<h3>COLMAP OUTPUT - Point Cloud (Custom Data) below</h3>
		<div id="container2"></div>
		<h3>MY OUTPUT - 3DGS - Splat (Extra Credit - Custom Data) below</h3>
		<div id="container3"></div>
		<!-- <h3>COLMAP OUTPUT - HERZ-JESUS-P8</h3>
		<div id="container4"></div>	 -->

		<p>The Neural Radiance Fields (NeRF) project is a breakthrough in computer graphics and 3D scene reconstruction. NeRF models scenes by learning a continuous function that predicts the color and density of a 3D point given its spatial coordinates.</p>
    

		<h2>Project details</h2>
		<ol>
			<li>Implemented and trained NeRF and 3DGS on Colab demo. </li>
			<li>Generated point clouds of nerf implementation and compared it with COLMAP point cloud. We see that NeRF has denser and more accurate 3d reconstruction.</li>
			<li>Trained on both inbuilt buldozer dataset and custom datset.</li>
		</ol>
			
    <h2>Algorithms Used</h2>
    <ol>
        <li><strong>NeRF Architecture:</strong> NeRF utilizes a neural network architecture that takes a 3D coordinate as input and outputs the color and density of the point at that location.</li>
        <li><strong>Ray Marching:</strong> NeRF employs ray marching to render scenes by shooting rays from the camera through each pixel and using the NeRF network to sample colors and densities along these rays.</li>
        <li><strong>Volume Rendering:</strong> Rendered images are created through volume rendering techniques, integrating sampled colors and densities along each ray to produce pixel values.</li>
        <li><strong>Loss Functions:</strong> NeRF minimizes losses such as rendering loss and regularization losses during training to ensure high-quality renderings and prevent overfitting.</li>
    </ol>
    
    <h2>Visualizations</h2>
    <ol>
        <li><strong>Rendered Images:</strong> Visualizations of NeRF-generated images showcase the high-quality rendering of scenes, including realistic lighting, shadows, and textures.</li>
        <li><strong>Depth Maps:</strong> Depth maps derived from NeRF can visualize scene geometry and depth perception, aiding in understanding the 3D structure of rendered scenes.</li>
        <li><strong>Rendering Progress:</strong> Visualizing the rendering progress during training shows how NeRF improves over time, starting from coarse approximations to detailed and realistic renderings.</li>
        <li><strong>Comparison:</strong> Visual comparisons between NeRF-rendered images and COLMAP highlight the model's accuracy in capturing scene details, colors, and lighting.</li>
    </ol>




        <!-- <div id="container1"></div> -->
		<script type="module" src="../js/assignment3.js"></script>
	</body>
</html>